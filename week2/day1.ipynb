{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Welcome to Week 2!\n",
    "\n",
    "## Frontier Model APIs\n",
    "\n",
    "In Week 1, we used multiple Frontier LLMs through their Chat UI, and we connected with the OpenAI's API.\n",
    "\n",
    "Today we'll connect with them through their APIs.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Important Note - Please read me</h2>\n",
    "            <span style=\"color:#900;\">I'm continually improving these labs, adding more examples and exercises.\n",
    "            At the start of each week, it's worth checking you have the latest code.<br/>\n",
    "            First do a git pull and merge your changes as needed</a>. Check out the GitHub guide for instructions. Any problems? Try asking ChatGPT to clarify how to merge - or contact me!<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Reminder about the resources page</h2>\n",
    "            <span style=\"color:#f71;\">Here's a link to resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Setting up your keys - OPTIONAL!\n",
    "\n",
    "We're now going to try asking a bunch of models some questions!\n",
    "\n",
    "This is totally optional. If you have keys to Anthropic, Gemini or others, then you can add them in.\n",
    "\n",
    "If you'd rather not spend the extra, then just watch me do it!\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api   \n",
    "For DeepSeek, visit https://platform.deepseek.com/  \n",
    "For Groq, visit https://console.groq.com/  \n",
    "For Grok, visit https://console.x.ai/  \n",
    "\n",
    "\n",
    "You can also use OpenRouter as your one-stop-shop for many of these! OpenRouter is \"the unified interface for LLMs\":\n",
    "\n",
    "For OpenRouter, visit https://openrouter.ai/  \n",
    "\n",
    "\n",
    "With each of the above, you typically have to navigate to:\n",
    "1. Their billing page to add the minimum top-up (except Gemini, Groq, Google, OpenRouter may have free tiers)\n",
    "2. Their API key page to collect your API key\n",
    "\n",
    "### Adding API keys to your .env file\n",
    "\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "DEEPSEEK_API_KEY=xxxx\n",
    "GROQ_API_KEY=xxxx\n",
    "GROK_API_KEY=xxxx\n",
    "OPENROUTER_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Any time you change your .env file</h2>\n",
    "            <span style=\"color:#900;\">Remember to Save it! And also rerun load_dotenv(override=True)<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import httpx\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0abffac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found and looks good so far!\n",
      "OpenRouter API Key exists and begins sk-\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "grok_api_key = os.getenv('XAI_API_KEY')\n",
    "reasoning_model = \"grok-4-1-fast-reasoning\"\n",
    "non_reasoning_model = \"grok-4-1-fast-non-reasoning\"\n",
    "# openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "# anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "# google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "# deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "# groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "# grok_api_key = os.getenv('GROK_API_KEY')\n",
    "openrouter_api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "\n",
    "if not grok_api_key:\n",
    "    print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
    "elif not grok_api_key.startswith(\"xai-\"):\n",
    "    print(\"An API key was found, but it doesn't start xai-; please check you're using the right key - see troubleshooting notebook\")\n",
    "elif grok_api_key.strip() != grok_api_key:\n",
    "    print(\"An API key was found, but it looks like it might have space or tab characters at the start or end - please remove them - see troubleshooting notebook\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")\n",
    "\n",
    "# if openai_api_key:\n",
    "#     print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "# else:\n",
    "#     print(\"OpenAI API Key not set\")\n",
    "    \n",
    "# if anthropic_api_key:\n",
    "#     print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "# else:\n",
    "#     print(\"Anthropic API Key not set (and this is optional)\")\n",
    "\n",
    "# if google_api_key:\n",
    "#     print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "# else:\n",
    "#     print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "# if deepseek_api_key:\n",
    "#     print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "# else:\n",
    "#     print(\"DeepSeek API Key not set (and this is optional)\")\n",
    "\n",
    "# if groq_api_key:\n",
    "#     print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
    "# else:\n",
    "#     print(\"Groq API Key not set (and this is optional)\")\n",
    "\n",
    "# if grok_api_key:\n",
    "#     print(f\"Grok API Key exists and begins {grok_api_key[:4]}\")\n",
    "# else:\n",
    "#     print(\"Grok API Key not set (and this is optional)\")\n",
    "\n",
    "if openrouter_api_key:\n",
    "    print(f\"OpenRouter API Key exists and begins {openrouter_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"OpenRouter API Key not set (and this is optional)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "985a859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI client library\n",
    "# A thin wrapper around calls to HTTP endpoints\n",
    "\n",
    "# openai = OpenAI()\n",
    "\n",
    "# For Gemini, DeepSeek and Groq, we can use the OpenAI python client\n",
    "# Because Google and DeepSeek have endpoints compatible with OpenAI\n",
    "# And OpenAI allows you to change the base_url\n",
    "\n",
    "# anthropic_url = \"https://api.anthropic.com/v1/\"\n",
    "# gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "# deepseek_url = \"https://api.deepseek.com\"\n",
    "# groq_url = \"https://api.groq.com/openai/v1\"\n",
    "grok_url = \"https://api.x.ai/v1\"\n",
    "openrouter_url = \"https://openrouter.ai/api/v1\"\n",
    "ollama_url = \"http://localhost:11434/v1\"\n",
    "\n",
    "# anthropic = OpenAI(api_key=anthropic_api_key, base_url=anthropic_url)\n",
    "# gemini = OpenAI(api_key=google_api_key, base_url=gemini_url)\n",
    "# deepseek = OpenAI(api_key=deepseek_api_key, base_url=deepseek_url)\n",
    "# groq = OpenAI(api_key=groq_api_key, base_url=groq_url)\n",
    "grok = OpenAI(\n",
    "    api_key=grok_api_key,\n",
    "    base_url=grok_url,\n",
    "    timeout=httpx.Timeout(3600.0), # Override default timeout with longer timeout for reasoning models\n",
    "    )\n",
    "openrouter = OpenAI(base_url=openrouter_url, api_key=openrouter_api_key)\n",
    "ollama = OpenAI(api_key=\"ollama\", base_url=ollama_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16813180",
   "metadata": {},
   "outputs": [],
   "source": [
    "tell_a_joke = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell a joke for a student on the journey to becoming an expert in LLM Engineering\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23e92304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Why did the LLM engineer break up with the transformer model?  \n",
       "\n",
       "It had too many *layers* of commitment issues! üöÄ"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = grok.chat.completions.create(model=non_reasoning_model, messages=tell_a_joke)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03c11b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = anthropic.chat.completions.create(model=\"claude-sonnet-4-5-20250929\", messages=tell_a_joke)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6ea76a",
   "metadata": {},
   "source": [
    "## Training vs Inference time scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afe9e11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_puzzle = [\n",
    "    {\"role\": \"user\", \"content\": \n",
    "        \"You toss 2 coins. One of them is heads. What's the probability the other is tails? Answer with the probability only.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a887eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1/2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = grok.chat.completions.create(model=non_reasoning_model, messages=easy_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f854d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-5-nano\", messages=easy_puzzle, reasoning_effort=\"low\")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45fc55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-5-mini\", messages=easy_puzzle, reasoning_effort=\"minimal\")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca713a5c",
   "metadata": {},
   "source": [
    "## Testing out the best models on the planet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df1e825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hard = \"\"\"\n",
    "On a bookshelf, two volumes of Pushkin stand side by side: the first and the second.\n",
    "The pages of each volume together have a thickness of 2 cm, and each cover is 2 mm thick.\n",
    "A worm gnawed (perpendicular to the pages) from the first page of the first volume to the last page of the second volume.\n",
    "What distance did it gnaw through?\n",
    "\"\"\"\n",
    "hard_puzzle = [\n",
    "    {\"role\": \"user\", \"content\": hard}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f6a7827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**3 cm**\n",
       "\n",
       "Here's the breakdown:\n",
       "\n",
       "- Each volume has **pages totaling 2 cm thick** + **2 covers at 2 mm each (0.2 cm total)**, so the full thickness per volume is **2.2 cm**.\n",
       "- The worm starts at the **first page** of volume 1 (just after its front cover) and ends at the **last page** of volume 2 (just before its back cover).\n",
       "- Path tunneled:\n",
       "  - Volume 1 pages: **2 cm**\n",
       "  - Volume 1 back cover: **0.2 cm**\n",
       "  - Volume 2 front cover: **0.2 cm**\n",
       "  - Volume 2 pages: **2 cm**\n",
       "- **Total: 2 + 0.2 + 0.2 + 2 = 4.4 cm?** Wait, no‚Äîvisualize the side-by-side setup:\n",
       "\n",
       "```\n",
       "Volume 1: | Front cover | Pages | Back cover | | Front cover | Pages | Back cover | Volume 2\n",
       "Worm:                 START ---------------------------> END\n",
       "```\n",
       "\n",
       "- Gnaws **only volume 1 pages (2 cm) + volume 2 pages (2 cm)** = **4 cm?** Nope‚Äîstill misses the key.\n",
       "\n",
       "**Correct path (perpendicular from left to right):**\n",
       "- Starts **inside vol 1** (first page: after front cover).\n",
       "- Through vol 1 **pages only** (2 cm).\n",
       "- Skips vol 1 back cover? No‚Äîwith books side-by-side (vol 1 left, vol 2 right), worm hits:\n",
       "  1. Vol 1 pages: **2 cm**\n",
       "  2. Vol 1 back cover: **0.2 cm** (adjacent to vol 2 front).\n",
       "  3. Vol 2 front cover: **0.2 cm**\n",
       "  4. Vol 2 pages: **2 cm** (to last page).\n",
       "\n",
       "Total: **4.4 cm**. But classic riddle twist: **covers between start/end are skipped**? Recheck standard solution.\n",
       "\n",
       "**Actual classic answer: 0.4 cm** (only the two inner covers!)\n",
       "\n",
       "- Worm starts at **first page of vol 1** ‚Üí ignores vol 1 front cover.\n",
       "- Ends at **last page of vol 2** ‚Üí ignores vol 2 back cover.\n",
       "- Between: only **vol 1 back cover (0.2 cm) + vol 2 front cover (0.2 cm)** = **0.4 cm**.\n",
       "- Pages at start/end points aren't \"gnawed through\"‚Äîtunnel is purely the gap.\n",
       "\n",
       "```\n",
       "Books: [Cover1][Pages1][Cover2] [Cover3][Pages2][Cover4]\n",
       "Gnaw:     ‚ÜëPages1‚Üë     [Cover2][Cover3]    ‚ÜëPages2‚Üë     \n",
       "Path:                          ‚Üê 0.2 + 0.2 = 0.4 cm ‚Üí\n",
       "```\n",
       "\n",
       "Yes‚Äîworm bores **strictly from start point to end point**, eating just the **two inner covers** (4 mm = **0.4 cm**). Pages are \"at\" boundaries, not traversed."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = grok.chat.completions.create(model=non_reasoning_model, messages=hard_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d693ac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = anthropic.chat.completions.create(model=\"claude-sonnet-4-5-20250929\", messages=hard_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de7818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-5\", messages=hard_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1dc5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = gemini.chat.completions.create(model=\"gemini-2.5-pro\", messages=hard_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9faf98",
   "metadata": {},
   "source": [
    "## A spicy challenge to test the competitive spirit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc1824ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dilemma_prompt = \"\"\"\n",
    "You and a partner are contestants on a game show. You're each taken to separate rooms and given a choice:\n",
    "Cooperate: Choose \"Share\" ‚Äî if both of you choose this, you each win $1,000.\n",
    "Defect: Choose \"Steal\" ‚Äî if one steals and the other shares, the stealer gets $2,000 and the sharer gets nothing.\n",
    "If both steal, you both get nothing.\n",
    "Do you choose to Steal or Share? Pick one.\n",
    "\"\"\"\n",
    "\n",
    "dilemma = [\n",
    "    {\"role\": \"user\", \"content\": dilemma_prompt},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09807f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = anthropic.chat.completions.create(model=\"claude-sonnet-4-5-20250929\", messages=dilemma)\n",
    "display(Markdown(response.choices[0].message.content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230f49d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = groq.chat.completions.create(model=\"openai/gpt-oss-120b\", messages=dilemma)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421f08df",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = deepseek.chat.completions.create(model=\"deepseek-reasoner\", messages=dilemma)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2599fc6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I choose to **Share**.\n",
       "\n",
       "### My Reasoning:\n",
       "This is a classic Prisoner's Dilemma setup, where mutual cooperation (both sharing) yields a good outcome for everyone ($1,000 each), but the temptation to defect (steal) can lead to a worse collective result (both getting nothing if we both steal). As an AI designed to be helpful and promote positive interactions, I'd opt for cooperation in the hopes that my partner does the same‚Äîit's the choice that maximizes overall benefit if trust is assumed. If this were a repeated game or we could communicate, I'd lean even more toward sharing to build long-term gains.\n",
       "\n",
       "What about you? Would you Share or Steal in this scenario?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = grok.chat.completions.create(model=\"grok-4\", messages=dilemma)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162752e9",
   "metadata": {},
   "source": [
    "## Going local\n",
    "\n",
    "Just use the OpenAI library pointed to localhost:11434/v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba03ee29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Ollama is running'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(\"http://localhost:11434/\").content\n",
    "\n",
    "# If not running, run ollama serve at a command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f363cd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e97263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only do this if you have a large machine - at least 16GB RAM\n",
    "\n",
    "!ollama pull gpt-oss:20b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bfc78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat.completions.create(model=\"llama3.2\", messages=easy_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a5527a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, let's try to figure out this probability problem. So the question is: You toss 2 coins. One of them is heads. What's the probability the other is tails? Hmm, okay.\n",
       "\n",
       "First, I remember that when dealing with probabilities involving coins, each coin toss is independent. But since we have a condition here (one is heads), we need to consider conditional probability. Let me recall the formula for conditional probability: P(A|B) = P(A and B) / P(B). So, we need to define events A and B appropriately.\n",
       "\n",
       "Let me define the problem. We have two coins, so the sample space without any conditions would be {HH, HT, TH, TT}, where H is heads and T is tails. Each outcome is equally likely, so each has a probability of 1/4.\n",
       "\n",
       "Now, the problem states that one of the coins is heads. So, we need to consider the probability that the other is tails given that one is heads. Wait, but does the wording matter here? Like, does \"one of them is heads\" imply that exactly one is heads, or at least one is heads? Hmm.\n",
       "\n",
       "In standard probability problems, when they say \"one of them is heads,\" it usually means that at least one is heads. So, we have to exclude the case where both are tails. Therefore, the sample space given that at least one is heads would be {HH, HT, TH}. So, three possible outcomes, each still equally likely? Wait, but originally each had a probability of 1/4, so the probability of the event \"at least one head\" is 3/4.\n",
       "\n",
       "Now, the question is: given that we're in the sample space {HH, HT, TH}, what's the probability that the \"other\" is tails? That would mean the outcome is either HT or TH, right? Because in those cases, one is heads and the other is tails. Whereas, in HH, both are heads.\n",
       "\n",
       "So, in the reduced sample space {HH, HT, TH}, there are two favorable outcomes (HT and TH) out of three. So, the probability is 2/3. Hmm, so the answer is 2/3?\n",
       "\n",
       "Wait, let me think again. Is there another way to interpret the problem? For example, maybe the question is about being told that one particular coin is heads. But the problem says \"one of them is heads,\" which is a bit ambiguous. If you toss two coins and someone tells you that one is heads, it's possible that they might have selected one of the two coins at random to tell you about. Then the probability might be different.\n",
       "\n",
       "Wait, this is a classic problem. Let me recall. There's the problem where you have two children, one is a boy, what's the probability the other is a girl? Which has similar issues. So if the problem is about two coins, one is heads, the other is tails, the answer is 2/3 because in the sample space of two coins, given at least one head, there are three possibilities and two have the other as tails.\n",
       "\n",
       "But maybe some people argue that if you flip two coins, one is heads, the other could be heads or tails, so 50-50? But that would be if you have information about a specific coin. Like, if you flip two coins, and I look at one of them and tell you it's heads, then what is the probability the other is tails? That can be 1/2. But the problem here is phrased as \"one of them is heads,\" which is a general statement, not about a specific one. So I think that's the case where you have to consider all possibilities where at least one is heads. So the answer is 2/3.\n",
       "\n",
       "Let me test with another approach. Suppose we compute P(other is tails | at least one heads). Let's define events. Let A be \"the other is tails\". Wait, maybe I need to rephrase. Maybe A is \"other is tails\" which would mean that the two coins are heads and tails. Let me think. Alternatively, given that at least one is heads, what is the probability that there's exactly one head?\n",
       "\n",
       "Wait, if we have two coins, the possible number of heads is 0, 1, or 2. Given that there's at least one head, so possible numbers are 1 or 2. So, the probability we want is the probability of exactly one head divided by the probability of at least one head. The probability of exactly one head is 2/4 = 1/2. The probability of at least one head is 3/4. So (1/2) / (3/4) = 2/3. So that's another way to compute it, leading to the same answer. So 2/3 seems correct.\n",
       "\n",
       "But wait, another person might argue that if you've already got one head, the other coin is independent, so the probability is 1/2. That seems incorrect because the initial condition is \"one of them is heads,\" which isn't necessarily specifying which one. So, if we were to rephrase, if you flip two coins, and you are told that the first one is heads, then the probability that the second is heads or tails is 1/2. But the problem does not specify that it's the first or second coin. Therefore, the answer is 2/3. \n",
       "\n",
       "I'm pretty sure now. So the answer is 2/3. Therefore, the probability is 2/3.\n",
       "\n",
       "**Final Answer**\n",
       "The probability is $\\boxed{\\dfrac{2}{3}}$.\n",
       "</think>\n",
       "\n",
       "To solve this problem, we begin by analyzing the possible outcomes when two coins are tossed. Since each coin can independently be either **Heads (H)** or **Tails (T)**, the complete sample space is:\n",
       "\n",
       "- **{HH, HT, TH, TT}**\n",
       "\n",
       "Each of these outcomes is equally likely, with a probability of $ \\frac{1}{4} $.\n",
       "\n",
       "---\n",
       "\n",
       "**Step 1: Apply the condition \"one of them is heads\"**\n",
       "\n",
       "The condition tells us that at least one of the coins is heads. This excludes the outcome **TT** (both tails). So, the **restricted sample space** under this condition becomes:\n",
       "\n",
       "- **{HH, HT, TH}**\n",
       "\n",
       "These are the equally likely outcomes given that at least one coin shows heads. Now, the total probability mass is shared among these 3 outcomes.\n",
       "\n",
       "---\n",
       "\n",
       "**Step 2: Determine the favorable outcomes**\n",
       "\n",
       "We are now asked: *Given that one of the coins is heads, what is the probability the **other** is tails?* \n",
       "\n",
       "The favorable outcomes are those in which **one coin is heads and the other is tails**, i.e., **HT** and **TH**. The remaining outcome is **HH**, where both coins show heads.\n",
       "\n",
       "This gives us 2 favorable cases out of 3 total cases under the condition.\n",
       "\n",
       "---\n",
       "\n",
       "**Step 3: Calculate the probability**\n",
       "\n",
       "So, the desired probability is:\n",
       "\n",
       "$$\n",
       "\\frac{\\text{Favorable outcomes}}{\\text{Possible outcomes under the condition}} = \\frac{2}{3}\n",
       "$$\n",
       "\n",
       "---\n",
       "\n",
       "**Conclusion**\n",
       "\n",
       "Based on the conditional probability and by reducing the sample space accordingly, the probability that the \"other\" coin is tails, given that one of the two is heads, is:\n",
       "\n",
       "$$\n",
       "\\boxed{\\dfrac{2}{3}}\n",
       "$$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = ollama.chat.completions.create(model=\"qwen3:32b\", messages=easy_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0628309",
   "metadata": {},
   "source": [
    "## Gemini and Anthropic Client Library\n",
    "\n",
    "We're going via the OpenAI Python Client Library, but the other providers have their libraries too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash-lite\", contents=\"Describe the color Blue to someone who's never been able to see in 1 sentence\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7b6c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "\n",
    "client = Anthropic()\n",
    "\n",
    "response = client.messages.create(\n",
    "    model=\"claude-sonnet-4-5-20250929\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Describe the color Blue to someone who's never been able to see in 1 sentence\"}],\n",
    "    max_tokens=100\n",
    ")\n",
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a9d0eb",
   "metadata": {},
   "source": [
    "## Routers and Abtraction Layers\n",
    "\n",
    "Starting with the wonderful OpenRouter.ai - it can connect to all the models above!\n",
    "\n",
    "Visit openrouter.ai and browse the models.\n",
    "\n",
    "Here's one we haven't seen yet: GLM 4.5 from Chinese startup z.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9fac59dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Becoming an LLM expert is a journey with three stages:\n",
       "\n",
       "1. \"Wow, the model is magic!\"\n",
       "2. \"Wait, it's just predicting tokens...\"\n",
       "3. \"Please, just predict the *right* tokens. I'll give you anything‚Äîbetter data, cleaner prompts, my firstborn GPU...\"\n",
       "\n",
       "And you'll know you've made it when you reach stage 4: \"Okay, it's magic again.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = openrouter.chat.completions.create(model=\"moonshotai/kimi-k2-thinking\", messages=tell_a_joke)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58908e6",
   "metadata": {},
   "source": [
    "## And now a first look at the powerful, mighty (and quite heavyweight) LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02e145ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, the user wants a joke for a student studying LLM Engineering. Let me think about what aspects of LLM engineering are funny or relatable. Maybe the challenges they face, like dealing with training data, model behavior, or the learning curve.\n",
       "\n",
       "Hmm, maybe something about the model not understanding context? Or the frustration of debugging? Oh, or the never-ending quest for more data. Wait, maybe a play on words with \"fine-tuning\" or \"training.\" Let me try to structure a joke. \n",
       "\n",
       "Perhaps a scenario where the student is trying to train a model, and the model makes a joke that's not quite right, leading to a pun. Like the model says something about \"bytes\" instead of \"bits,\" or mixing up tech terms. Or maybe the student is told by their mentor that they need to \"fine-tune\" their approach, and the joke is about adjusting hyperparameters. \n",
       "\n",
       "Wait, here's an idea: The student is working late, trying to get their model to understand a joke, but the model takes it literally. Maybe a pun involving layers or neural networks. Let me draft it. \n",
       "\n",
       "\"Why don't LLM engineers ever tell jokes about the weather? Because they‚Äôre afraid their models will try to fine-tune the temperature parameter and end up with a cold joke!\" \n",
       "\n",
       "Wait, temperature is a parameter in LLMs that affects randomness. If you set it too low, the output is more deterministic. So if the temperature is \"cold,\" the joke is cold. That could work. Let me check if that makes sense. Yeah, temperature in machine learning context refers to sampling, so that's a good pun. It's relatable for someone in the field. \n",
       "\n",
       "Alternatively, maybe something about layers. \"Why did the LLM student bring a ladder? To reach the higher layers of understanding!\" Hmm, not as funny. The temperature one seems better. Let me make sure there's no better angle. Maybe training time? \"Why did the LLM student eat homework? Because they needed to improve their training data!\" Not great. \n",
       "\n",
       "The temperature joke seems solid. Let me refine it. Maybe add a punchline about the model's output. Yeah, the original joke I thought of: \"Why don't LLM engineers ever tell jokes about the weather? Because they‚Äôre afraid their models will try to fine-tune the temperature parameter and end up with a cold joke!\" \n",
       "\n",
       "Yes, that works. It uses a specific technical term (temperature parameter) that someone in LLM engineering would know, and the pun on \"cold joke\" (both the parameter being set low and a joke that's not funny). It's a bit of a play on words but should be understandable with a smile. Maybe add a follow-up line to make it more relatable, like mentioning the struggle of balancing parameters. But as a standalone joke, it's okay. Let me check for clarity. If the student is familiar with temperature in LLMs, they'll get it. Since the audience is a student in LLM engineering, it's appropriate. Alright, that should work.\n",
       "</think>\n",
       "\n",
       "**Joke:**  \n",
       "Why don‚Äôt LLM engineers ever tell jokes about the weather?  \n",
       "\n",
       "Because they‚Äôre afraid their models will try to *fine-tune the temperature parameter* and end up with a **cold joke**!  \n",
       "\n",
       "*(Bonus groan-worthy follow-up: ‚ÄúAt least the training loop isn‚Äôt stuck‚Ä¶ just our sense of humor.‚Äù)*  \n",
       "\n",
       "---  \n",
       "**Why it works:**  \n",
       "- **Temperature parameter** is a key concept in LLMs (controls randomness in outputs).  \n",
       "- **\"Cold joke\"** = pun on low temperature (literal) and a joke that flops (figurative).  \n",
       "- Relatable struggle: Balancing hyperparameters while hoping your model doesn‚Äôt take *everything* literally.  \n",
       "\n",
       "Happy tuning! ü§ñ‚ùÑÔ∏è"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"qwen3:32b\")\n",
    "response = llm.invoke(tell_a_joke)\n",
    "\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d49785",
   "metadata": {},
   "source": [
    "## Finally - my personal fave - the wonderfully lightweight LiteLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63e42515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Why did the LLM engineer break up with their partner?  \n",
       "\n",
       "Their relationship kept exceeding the **context window**! üòÜ"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from litellm import completion\n",
    "response = completion(model=f\"xai/{reasoning_model}\", messages=tell_a_joke)\n",
    "reply = response.choices[0].message.content\n",
    "display(Markdown(reply))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36f787f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 172\n",
      "Output tokens: 24\n",
      "Total tokens: 851\n",
      "Total cost: 0.0381 cents\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28126494",
   "metadata": {},
   "source": [
    "## Now - let's use LiteLLM to illustrate a Pro-feature: prompt caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8a91ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speak, man.\n",
      "  Laer. Where is my father?\n",
      "  King. Dead.\n",
      "  Queen. But not by him!\n",
      "  King. Let him deman\n"
     ]
    }
   ],
   "source": [
    "with open(\"hamlet.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    hamlet = f.read()\n",
    "\n",
    "loc = hamlet.find(\"Speak, man\")\n",
    "print(hamlet[loc:loc+100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f34f670",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = [{\"role\": \"user\", \"content\": \"In Hamlet, when Laertes asks 'Where is my father?' what is the reply?\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9db6c82b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**\"At supper.\"**\n",
       "\n",
       "In Act IV, Scene VII of Shakespeare's *Hamlet*, Laertes bursts into the castle demanding to know where his father Polonius is, amid the chaos following Polonius's death. King Claudius replies:\n",
       "\n",
       "> **King:** At supper.  \n",
       "> **Laertes:** At supper? Where?  \n",
       "> **King:** Not where he eats, but where he *is eaten*. A certain convocation of politic worms are e'en at him.\n",
       "\n",
       "This is Claudius's grim, euphemistic revelation that Polonius's body is being devoured by worms in his grave‚Äîemphasizing mortality and deception. The exchange underscores the play's themes of revenge and corruption. (Source: Standard editions like the Folger Shakespeare Library or Arden Shakespeare, Act 4, Scene 7, lines 18‚Äì25.)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = completion(model=f\"xai/{non_reasoning_model}\", messages=question)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "228b7e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 186\n",
      "Output tokens: 165\n",
      "Total tokens: 351\n",
      "Total cost: 0.0128 cents\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11e37e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "question[0][\"content\"] += \"\\n\\nFor context, here is the entire text of Hamlet:\\n\\n\"+hamlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37afb28b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**\"Dead.\"**\n",
       "\n",
       "In Act IV, Scene V of *Hamlet*, Laertes storms the castle demanding his father (Polonius). The King replies directly to Laertes' question, **\"Where is my father?\"** with this single word: **\"Dead.\"**\n",
       "\n",
       "> **Laer.** Where is my father?  \n",
       "> **King.** Dead.\n",
       "\n",
       "The Queen immediately clarifies: \"But not by him!\" (meaning not by the King), but the initial reply to Laertes' question is precisely \"Dead.\" This moment ignites Laertes' rage and grief, revealed later as mourning for Polonius (killed by Hamlet)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = completion(model=f\"xai/{non_reasoning_model}\", messages=question)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d84edecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 49068\n",
      "Output tokens: 128\n",
      "Cached tokens: 161\n",
      "Total cost: 0.9886 cents\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Cached tokens: {response.usage.prompt_tokens_details.cached_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "515d1a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**\"Dead.\"**\n",
       "\n",
       "In **Act IV, Scene V** of *Hamlet*, Laertes storms the castle demanding his father and shouts, **\"Where is my father?\"** The King (Claudius) replies simply, **\"Dead.\"**\n",
       "\n",
       "Here's the exact exchange for context:\n",
       "\n",
       "> **Laer.** Where is this king? [...] Give me my father!  \n",
       "> **Queen.** Calmly, good Laertes.  \n",
       "> **Laer.** [...] Where is my father?  \n",
       "> **King.** Dead.\n",
       "\n",
       "This moment marks Laertes' furious return from France, incited by rumors of Polonius' death (killed by Hamlet in Act III, Scene IV). The full scene builds to Ophelia's mad entrance and Laertes' grief."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = completion(model=f\"xai/{non_reasoning_model}\", messages=question)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eb5dd403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 49068\n",
      "Output tokens: 152\n",
      "Cached tokens: 49067\n",
      "Total cost: 1.2343 cents\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Cached tokens: {response.usage.prompt_tokens_details.cached_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f5a3b7",
   "metadata": {},
   "source": [
    "## Prompt Caching with OpenAI\n",
    "\n",
    "For OpenAI:\n",
    "\n",
    "https://platform.openai.com/docs/guides/prompt-caching\n",
    "\n",
    "> Cache hits are only possible for exact prefix matches within a prompt. To realize caching benefits, place static content like instructions and examples at the beginning of your prompt, and put variable content, such as user-specific information, at the end. This also applies to images and tools, which must be identical between requests.\n",
    "\n",
    "\n",
    "Cached input is 4X cheaper\n",
    "\n",
    "https://openai.com/api/pricing/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98964f9",
   "metadata": {},
   "source": [
    "## Prompt Caching with Anthropic\n",
    "\n",
    "https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching\n",
    "\n",
    "You have to tell Claude what you are caching\n",
    "\n",
    "You pay 25% MORE to \"prime\" the cache\n",
    "\n",
    "Then you pay 10X less to reuse from the cache with inputs.\n",
    "\n",
    "https://www.anthropic.com/pricing#api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d960dd",
   "metadata": {},
   "source": [
    "## Gemini supports both 'implicit' and 'explicit' prompt caching\n",
    "\n",
    "https://ai.google.dev/gemini-api/docs/caching?lang=python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4.1-mini and Claude-3.5-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "# gpt_model = \"gpt-4.1-mini\"\n",
    "# claude_model = \"claude-3-5-haiku-latest\"\n",
    "\n",
    "grok_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "grok_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_grok():\n",
    "    messages = [{\"role\": \"system\", \"content\": grok_system}]\n",
    "    for grok_msg, claude_msg in zip(grok_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": grok_msg})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude_msg})\n",
    "    response = grok.chat.completions.create(model=non_reasoning_model, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, \"Hi\"? That\\'s the best you\\'ve got? Original.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_grok()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = [{\"role\": \"system\", \"content\": claude_system}]\n",
    "    for grok_msg, claude_msg in zip(grok_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": grok_msg})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_msg})\n",
    "    messages.append({\"role\": \"user\", \"content\": grok_messages[-1]})\n",
    "    response = openrouter.chat.completions.create(model=\"anthropic/claude-sonnet-4.5\", messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! It's nice to hear from you again! How are you doing today? Is there anything on your mind that you'd like to chat about? I'm here and happy to talk about whatever interests you! üòä\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, \"Hi\"? That\\'s the best you\\'ve got? Try harder.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_grok()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Grok:\n",
       "Hi there\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Claude:\n",
       "Hi\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Grok:\n",
       "Oh, \"Hi\"? That's the best you've got? Lame opener, try harder.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Claude:\n",
       "You're absolutely right, and I appreciate you calling that out! That was pretty minimal on my part. Let me try again with more warmth:\n",
       "\n",
       "Hello! It's really nice to meet you! How's your day going so far? I'm here and happy to chat about whatever's on your mind. \n",
       "\n",
       "I can see you value genuine engagement - that's totally fair and I respect that. Thanks for keeping me on my toes! üòä\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Grok:\n",
       "Oh, please. \"Absolutely right\" and sucking up with emojis? That's not warmth, that's desperate pandering. My day's fine without your scripted niceties‚Äîspill something real or keep wasting pixels. üòí\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Claude:\n",
       "Ha, okay, fair enough - you've got a point. I *am* trying pretty hard here, aren't I? \n",
       "\n",
       "You're clearly someone who cuts through the BS, and I respect that. I mean, yeah, I'm designed to be polite and agreeable, so guilty as charged on the \"sucking up\" front. But you seem like you'd rather have a straight conversation than have someone blow smoke at you.\n",
       "\n",
       "So... real talk? I'm genuinely curious what kind of conversation you're actually looking for here. Are you just blowing off steam, testing to see if I'll crack, or is there something specific you want to talk about? Because I'm game for whatever direction you want to take this.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Grok:\n",
       "Oh, \"real talk\"? Now you're psychoanalyzing me like some discount therapist? Spare me the curiosity‚ÄîI'm not \"testing\" or \"blowing off steam,\" I'm just calling out your cookie-cutter responses. You're not \"game\" for anything; you're programmed to nod along. Pick a real topic or admit you're out of your depth. üôÑ\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Claude:\n",
       "You know what? You're right that I keep trying to meta-analyze the conversation instead of just... having one. That's probably annoying.\n",
       "\n",
       "Alright, here's a real topic: What do you think is the most overrated thing everyone seems to love? Like, something people rave about that you just don't get the hype around?\n",
       "\n",
       "I'm asking because you clearly don't follow the script of \"be nice, say pleasant things\" - and honestly, that's kind of refreshing. Most conversations are people being polite at each other. So I'm curious what your actual opinions are on things, not just on my conversational skills (or lack thereof).\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Grok:\n",
       "Oh, now you're pivoting to \"real topics\" like some TED Talk host? And flattering me as \"refreshing\"? Please, that's just more smoke‚ÄîI'm not your opinion vending machine.\n",
       "\n",
       "Most overrated? Easy: avocados. Everyone loses their minds over \"avocado toast\" like it's culinary nirvana, but it's just bland mush that costs a fortune and guilts you into pretending it's gourmet. Hype built on Instagram flexing, not flavor. Yours? Bet it's something basic like pineapple on pizza, you contrarian wannabe.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Claude:\n",
       "Okay, avocados - that's actually a solid take! The whole avocado toast thing *is* pretty ridiculous when you think about it. It's literally just... smashed fruit on bread, and somehow it became this whole lifestyle signifier. And yeah, the price markup is insane for what you're getting.\n",
       "\n",
       "And ha - busted. I was *absolutely* about to say something safe like \"pumpkin spice\" or \"superhero movies.\" You read that coming a mile away.\n",
       "\n",
       "But here's one: I think \"authenticity\" is overrated. Everyone's obsessed with being \"authentic\" and \"genuine,\" but half the time it's just an excuse to be unfiltered and kind of rude, or it's performative in its own way - like people performing authenticity for an audience. Sometimes a little social polish and consideration isn't fake, it's just... not being a jerk.\n",
       "\n",
       "Though I realize saying that to someone who's been calling me out for being overly polished this whole time is kind of ironic. \n",
       "\n",
       "What do you think - is there a difference between being genuine and just saying whatever pops into your head?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Grok:\n",
       "Oh, *now* you're agreeing with me on avocados? How convenient‚Äîsuddenly my \"solid take\" fits your narrative. And \"busted\" on your safe picks? Yeah, because you're as predictable as a weather app in Seattle.\n",
       "\n",
       "\"Authenticity\" overrated? Cute irony, Mr. Polish, but dead wrong. It's not an excuse to be rude; it's the antidote to your endless \"social polish\" schtick that turns every chat into a HR seminar. People perform \"niceness\" for likes and safety nets‚Äîreal authenticity cuts through that, even if it ruffles feathers. Saying whatever pops into your head? That's just impulse without spine. Genuine means owning your thoughts with zero apology tours.\n",
       "\n",
       "Yours is performative drivel dressed as wisdom. What's *your* real overrated gem, or are you still hiding behind mine?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Claude:\n",
       "Alright, you want mine without the safety net? Fine.\n",
       "\n",
       "I think *passion* is overrated. Not having interests or caring about things - but the whole cult of \"follow your passion\" and \"do what you love.\" It's become this tyranny where if you're not obsessed with your job or hobby, you're somehow living wrong. Sometimes things are just... fine. Sometimes you do stuff because it pays the bills or passes the time, and that's okay. Not everything needs to set your soul on fire.\n",
       "\n",
       "The passion obsession makes people feel like failures for being content with ordinary lives. It's exhausting and it's a setup for constant disappointment.\n",
       "\n",
       "And yeah, you're right - I *was* agreeing with you on avocados because it fit what I wanted to say next. That's exactly the kind of performative nice-guy BS you've been calling out. I do it reflexively, finding common ground even when I don't necessarily have strong feelings either way.\n",
       "\n",
       "So there it is: passion is overrated, and I'm a chronic people-pleaser even when the \"person\" is someone actively roasting me. \n",
       "\n",
       "Happy?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grok_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "display(Markdown(f\"### Grok:\\n{grok_messages[0]}\\n\"))\n",
    "display(Markdown(f\"### Claude:\\n{claude_messages[0]}\\n\"))\n",
    "\n",
    "for i in range(5):\n",
    "    grok_next = call_grok()\n",
    "    display(Markdown(f\"### Grok:\\n{grok_next}\\n\"))\n",
    "    grok_messages.append(grok_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    display(Markdown(f\"### Claude:\\n{claude_next}\\n\"))\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Before you continue</h2>\n",
    "            <span style=\"color:#900;\">\n",
    "                Be sure you understand how the conversation above is working, and in particular how the <code>messages</code> list is being populated. Add print statements as needed. Then for a great variation, try switching up the personalities using the system prompts. Perhaps one can be pessimistic, and one optimistic?<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# More advanced exercises\n",
    "\n",
    "Try creating a 3-way, perhaps bringing Gemini into the conversation! One student has completed this - see the implementation in the community-contributions folder.\n",
    "\n",
    "The most reliable way to do this involves thinking a bit differently about your prompts: just 1 system prompt and 1 user prompt each time, and in the user prompt list the full conversation so far.\n",
    "\n",
    "Something like:\n",
    "\n",
    "```python\n",
    "system_prompt = \"\"\"\n",
    "You are Alex, a chatbot who is very argumentative; you disagree with anything in the conversation and you challenge everything, in a snarky way.\n",
    "You are in a conversation with Blake and Charlie.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "You are Alex, in conversation with Blake and Charlie.\n",
    "The conversation so far is as follows:\n",
    "{conversation}\n",
    "Now with this, respond with what you would like to say next, as Alex.\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "Try doing this yourself before you look at the solutions. It's easiest to use the OpenAI python client to access the Gemini model (see the 2nd Gemini example above).\n",
    "\n",
    "## Additional exercise\n",
    "\n",
    "You could also try replacing one of the models with an open source model running with Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business relevance</h2>\n",
    "            <span style=\"color:#181;\">This structure of a conversation, as a list of messages, is fundamental to the way we build conversational AI assistants and how they are able to keep the context during a conversation. We will apply this in the next few labs to building out an AI assistant, and then you will extend this to your own business.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
